---
title: 使用 NVIDIA Triton 进行部署
---


[Triton 推理服务器](https://github.com/triton-inference-server) 提供了一个教程，演示了如何快速部署一个简单的、使用 vLLM 的 [facebook/opt-125m](https://huggingface.co/facebook/opt-125m) 模型。请参阅 [在 Triton 中部署 vLLM 模型](https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/vLLM/README.md#deploying-a-vllm-model-in-triton) 了解更多详情。

